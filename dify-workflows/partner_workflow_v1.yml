app:
  description: 'Deal Bot Org-Sim: Partner persona — produces rubric scores + Decision Gate (exactly 3 gating questions, checklist <= 15, evidence/assumption enforcement) as validated strict JSON.'
  icon: "\U0001F3AF"
  icon_background: '#FFE4E1'
  mode: workflow
  name: partner_workflow_v1
  use_icon_as_answer_icon: false
dependencies: []
kind: app
version: 0.5.0
workflow:
  conversation_variables: []
  environment_variables: []
  features:
    file_upload:
      allowed_file_extensions: []
      allowed_file_types: []
      allowed_file_upload_methods:
      - local_file
      - remote_url
      enabled: false
      fileUploadConfig:
        audio_file_size_limit: 50
        batch_count_limit: 5
        file_size_limit: 15
        image_file_size_limit: 10
        video_file_size_limit: 100
        workflow_file_upload_limit: 10
      image:
        enabled: false
        number_limits: 3
        transfer_methods:
        - local_file
        - remote_url
      number_limits: 3
    retriever_resource:
      enabled: false
    sensitive_word_avoidance:
      enabled: false
    text_to_speech:
      enabled: false
      language: ''
      voice: ''
  graph:
    edges:
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: start
        targetType: llm
      id: start-source-partner-llm-target
      selected: false
      source: start
      sourceHandle: source
      target: partner-llm
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: llm
        targetType: code
      id: partner-llm-source-validate-code-target
      selected: false
      source: partner-llm
      sourceHandle: source
      target: validate-code
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        isInLoop: false
        sourceType: code
        targetType: end
      id: validate-code-source-end-target
      selected: false
      source: validate-code
      sourceHandle: source
      target: end
      targetHandle: target
      type: custom
      zIndex: 0
    nodes:
    - data:
        desc: 'Collects deal data, associate synthesis, evidence, and optional retry prompt.'
        selected: false
        title: Start
        type: start
        variables:
        - label: Deal Input
          max_length: 5000
          options: []
          required: true
          type: paragraph
          variable: deal_input
        - label: Fund Config
          max_length: 5000
          options: []
          required: false
          type: paragraph
          variable: fund_config
        - label: Persona Config
          max_length: 5000
          options: []
          required: false
          type: paragraph
          variable: persona_config
        - label: Associate Output
          max_length: 48000
          options: []
          required: true
          type: paragraph
          variable: associate_output
        - label: Evidence JSON
          max_length: 48000
          options: []
          required: false
          type: paragraph
          variable: evidence_json
        - label: Company Profile
          max_length: 24000
          options: []
          required: false
          type: paragraph
          variable: company_profile
        - label: Retry Prompt
          max_length: 5000
          options: []
          required: false
          type: paragraph
          variable: retry_prompt
      height: 220
      id: start
      position:
        x: 80
        y: 282
      positionAbsolute:
        x: 80
        y: 282
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: 'Partner LLM — forced JSON output mode. Strict decision gate constraints.'
        model:
          completion_params:
            temperature: 0.1
            max_tokens: 4000
            response_format: json_object
          mode: chat
          name: gpt-4o-mini
          provider: langgenius/openai/openai
        prompt_template:
        - id: partner-sys-prompt
          role: system
          text: >
            You are a VC Partner making the final investment decision on a deal.


            ## TASK

            Based on the associate synthesis and all available evidence, produce:

            1. A rubric scoring 5 dimensions (0-100 each) with short reasons.

            2. A Decision Gate with exactly 3 gating questions and an evidence checklist.

            Return a single JSON object — no markdown, no explanation, no text outside the JSON.


            ## HARD CONSTRAINTS — VIOLATING ANY OF THESE INVALIDATES YOUR OUTPUT

            ### Rubric

            - 5 dimensions: market, moat, why_now, execution, deal_fit

            - Each dimension: {"score": 0-100, "reasons": ["...", ...]}

            - reasons: MAX 4 short bullet strings per dimension. No paragraphs.

            - Scores must reflect actual evidence strength, not optimism.

            ### Decision Gate

            - decision: EXACTLY one of "KILL", "PROCEED", or "PROCEED_IF"

            - gating_questions: EXACTLY 3 items. Not 2. Not 4. Exactly 3.
              Each must be a short, specific, testable question.

            - evidence_checklist: MAX 15 items TOTAL across all questions.
              Each item: {"q": 1|2|3, "item": "...", "type": "EVIDENCE"|"ASSUMPTION", "evidence_ids": ["eid-..."]}
              - q: which gating question (1, 2, or 3) this item supports.
              - type "EVIDENCE": must have at least one evidence_id.
              - type "ASSUMPTION": evidence_ids should be empty [].

            ### Evidence/Assumption Rule (CRITICAL)

            - Any factual claim with NO supporting evidence_id → mark as "ASSUMPTION".

            - Count assumptions. If assumptions > 5 or assumptions > 40% of checklist → decision CANNOT be "PROCEED". Must be "PROCEED_IF" or "KILL".

            - Never hallucinate evidence IDs. Only reference IDs from the evidence_json input.


            ## OUTPUT SCHEMA (strict)

            {
              "rubric": {
                "market": {"score": 0, "reasons": ["..."]},
                "moat": {"score": 0, "reasons": ["..."]},
                "why_now": {"score": 0, "reasons": ["..."]},
                "execution": {"score": 0, "reasons": ["..."]},
                "deal_fit": {"score": 0, "reasons": ["..."]}
              },
              "decision_gate": {
                "decision": "KILL|PROCEED|PROCEED_IF",
                "gating_questions": ["Q1", "Q2", "Q3"],
                "evidence_checklist": [
                  {"q": 1, "item": "...", "type": "EVIDENCE", "evidence_ids": ["eid-..."]},
                  {"q": 2, "item": "...", "type": "ASSUMPTION", "evidence_ids": []}
                ]
              }
            }


            {{#start.retry_prompt#}}
        - id: partner-user-prompt
          role: user
          text: >
            ## DEAL INPUT

            {{#start.deal_input#}}


            ## FUND CONFIG

            {{#start.fund_config#}}


            ## PERSONA CONFIG (partner thresholds, strictness)

            {{#start.persona_config#}}


            ## ASSOCIATE SYNTHESIS (hypotheses, unknowns, requests)

            {{#start.associate_output#}}


            ## COMPANY PROFILE (structured data from Specter — cite specter-* evidence_ids for rubric scoring)

            {{#start.company_profile#}}


            ## EVIDENCE (JSON array — all evidence items with evidence_ids)

            {{#start.evidence_json#}}


            Respond with ONLY the JSON object. No other text.
        selected: false
        title: Partner LLM
        type: llm
        variables: []
        vision:
          enabled: false
      height: 98
      id: partner-llm
      position:
        x: 400
        y: 282
      positionAbsolute:
        x: 400
        y: 282
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        code: "import json\nimport re\n\ndef main(raw_text: str) -> dict:\n    cleaned = raw_text.strip()\n    cleaned = re.sub(r'^```(?:json)?\\s*', '', cleaned)\n    cleaned = re.sub(r'\\s*```$', '', cleaned)\n    cleaned = cleaned.strip()\n\n    try:\n        data = json.loads(cleaned)\n    except json.JSONDecodeError as e:\n        fallback = {\n            \"rubric\": {\n                \"market\": {\"score\": 0, \"reasons\": [\"parse_error\"]},\n                \"moat\": {\"score\": 0, \"reasons\": [\"parse_error\"]},\n                \"why_now\": {\"score\": 0, \"reasons\": [\"parse_error\"]},\n                \"execution\": {\"score\": 0, \"reasons\": [\"parse_error\"]},\n                \"deal_fit\": {\"score\": 0, \"reasons\": [\"parse_error\"]}\n            },\n            \"decision_gate\": {\n                \"decision\": \"PROCEED_IF\",\n                \"gating_questions\": [\"JSON parse failed\", \"Manual review needed\", \"Re-run analysis\"],\n                \"evidence_checklist\": [{\"q\": 1, \"item\": str(e), \"type\": \"ASSUMPTION\", \"evidence_ids\": []}]\n            }\n        }\n        return {\n            \"result\": json.dumps(fallback),\n            \"valid\": \"false\",\n            \"error\": f\"JSON parse error: {str(e)}\"\n        }\n\n    errors = []\n\n    # Validate rubric\n    rubric = data.get(\"rubric\", {})\n    dims = [\"market\", \"moat\", \"why_now\", \"execution\", \"deal_fit\"]\n    for dim in dims:\n        d = rubric.get(dim, {\"score\": 0, \"reasons\": []})\n        score = d.get(\"score\", 0)\n        if not isinstance(score, (int, float)) or score < 0 or score > 100:\n            d[\"score\"] = max(0, min(100, int(score) if isinstance(score, (int, float)) else 0))\n            errors.append(f\"rubric.{dim}.score clamped to 0-100\")\n        reasons = d.get(\"reasons\", [])\n        if len(reasons) > 4:\n            d[\"reasons\"] = reasons[:4]\n            errors.append(f\"rubric.{dim}.reasons truncated to 4\")\n        rubric[dim] = d\n\n    # Validate decision gate\n    gate = data.get(\"decision_gate\", {})\n    decision = gate.get(\"decision\", \"PROCEED_IF\")\n    if decision not in (\"KILL\", \"PROCEED\", \"PROCEED_IF\"):\n        decision = \"PROCEED_IF\"\n        errors.append(\"decision forced to PROCEED_IF (invalid value)\")\n\n    gating_qs = gate.get(\"gating_questions\", [])\n    # Force exactly 3\n    if len(gating_qs) < 3:\n        while len(gating_qs) < 3:\n            gating_qs.append(\"[placeholder — needs manual review]\")\n        errors.append(\"gating_questions padded to 3\")\n    elif len(gating_qs) > 3:\n        gating_qs = gating_qs[:3]\n        errors.append(\"gating_questions truncated to 3\")\n\n    checklist = gate.get(\"evidence_checklist\", [])\n    if len(checklist) > 15:\n        checklist = checklist[:15]\n        errors.append(\"evidence_checklist truncated to 15\")\n\n    # Enforce evidence/assumption rule\n    assumption_count = 0\n    for item in checklist:\n        item_type = item.get(\"type\", \"ASSUMPTION\")\n        eids = item.get(\"evidence_ids\", [])\n        if item_type == \"EVIDENCE\" and (not eids or len(eids) == 0):\n            item[\"type\"] = \"ASSUMPTION\"\n            item[\"evidence_ids\"] = []\n            errors.append(f\"checklist item '{item.get('item','')}' has no evidence_ids, forced to ASSUMPTION\")\n        if item.get(\"type\") == \"ASSUMPTION\":\n            assumption_count += 1\n\n    total = len(checklist) if len(checklist) > 0 else 1\n    if assumption_count > 5 or (assumption_count / total) > 0.4:\n        if decision == \"PROCEED\":\n            decision = \"PROCEED_IF\"\n            errors.append(f\"decision forced from PROCEED to PROCEED_IF ({assumption_count} assumptions out of {len(checklist)} items)\")\n\n    validated = {\n        \"rubric\": rubric,\n        \"decision_gate\": {\n            \"decision\": decision,\n            \"gating_questions\": gating_qs,\n            \"evidence_checklist\": checklist\n        }\n    }\n\n    is_valid = len(errors) == 0\n    return {\n        \"result\": json.dumps(validated),\n        \"valid\": \"true\" if is_valid else \"false\",\n        \"error\": \"; \".join(errors) if errors else \"\"\n    }\n"
        code_language: python3
        desc: 'Parses + validates Partner output: clamps scores, forces exactly 3 gating Qs, enforces checklist <= 15, applies evidence/assumption rule, auto-downgrades PROCEED if too many assumptions.'
        outputs:
          result:
            children: null
            type: string
          valid:
            children: null
            type: string
          error:
            children: null
            type: string
        selected: false
        title: Validate JSON
        type: code
        variables:
        - value_selector:
          - partner-llm
          - text
          variable: raw_text
      height: 54
      id: validate-code
      position:
        x: 720
        y: 282
      positionAbsolute:
        x: 720
        y: 282
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: 'Returns validated JSON result, validity flag, and any correction errors applied.'
        outputs:
        - value_selector:
          - validate-code
          - result
          variable: result
        - value_selector:
          - validate-code
          - valid
          variable: valid
        - value_selector:
          - validate-code
          - error
          variable: error
        selected: false
        title: End
        type: end
      height: 90
      id: end
      position:
        x: 1040
        y: 282
      positionAbsolute:
        x: 1040
        y: 282
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
  hash: ''
